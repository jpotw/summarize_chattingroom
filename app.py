import datetime
import os
from delete import make_new_file
import streamlit as st
from langchain.llms import openai
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from langchain.chains import RetrievalQA
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from extract_urls import extract_urls
import streamlit_scrollable_textbox as stx


#API í‚¤ ì„¸íŒ…
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")


def generate_response(new_file, query_text):
        # Split document into chunks
        text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=20)
        texts = text_splitter.create_documents([new_file])  # Ensure it's a list

        # Check if texts is empty
        if not texts:
            raise ValueError("No texts were generated from the document.")
        # Select embeddings
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        # Create a vectorstore from documents
        db = Chroma.from_documents(texts, embeddings)
        # Create retriever interface
        retriever = db.as_retriever()
        # Create QA chain
        qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(openai_api_key=openai_api_key), chain_type='stuff', retriever=retriever)
        return qa.run(query_text)


##################################################################################################################################################
# ì—¬ê¸°ì„œë¶€í„°ëŠ” ì¸í„°í˜ì´ìŠ¤(streamlit)
## íƒ€ì´í‹€
st.title("ğŸ— ì˜¤í”ˆ ì±„íŒ…ë°©ì˜ í•˜ë£¨ì¹˜ ë‚´ìš©ì„ ìš”ì•½í•´ë“œë¦½ë‹ˆë‹¤.")


uploaded_file = st.file_uploader('ì¹´ì¹´ì˜¤í†¡ txtíŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.', type='txt')

if uploaded_file is not None:
    with st.spinner('íŒŒì¼ì„ ìë¥´ëŠ” ì¤‘ì…ë‹ˆë‹¤.'):
        new_file = make_new_file(uploaded_file)
        st.write("íŒŒì¼ì„ ì˜ëìŠµë‹ˆë‹¤.")
        if st.button('ê¿€í†µ ì¶”ì¶œí•˜ê¸°'):
            clickable_urls = extract_urls(new_file)
            st.markdown(clickable_urls, unsafe_allow_html=True)
            # st.write(("URL ëª¨ìŒ:"))
            # for url in extracted_urls:
            #     st.write(url)
else:
    st.error("ì•„ì§ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


## í”„ë¡¬í”„íŠ¸ ë§Œë“¤ê¸°
with st.form('í”„ë¡¬í”„íŠ¸ ë§Œë“¤ê¸°', clear_on_submit=True):
    chattingroom = st.text_input('ì±„íŒ…ë°© ì´ë¦„', type='default')
    content = st.text_input('ì±„íŒ…ë°©ì˜ ì£¼ì œë¥¼ ì ì–´ì£¼ì„¸ìš”. ex. ì£¼ì‹', type='default')
    # openai_api_key = st.text_input('OpenAI API Key', type='password')
    submitted = st.form_submit_button('ì…ë ¥')
    if submitted:
        stx.scrollableTextbox(f"""

    ì´ íŒŒì¼ì€ "{chattingroom}"ì´ë¼ëŠ” ì±„íŒ…ë°©ì˜ ê¸°ë¡ì…ë‹ˆë‹¤.
    í•´ë‹¹ ì±„íŒ…ë°©ì—ì„œëŠ” ì£¼ë¡œ "{content}"ì„ ì£¼ì œë¡œ ì´ì•¼ê¸°í•©ë‹ˆë‹¤.

    ---

    í•´ë‹¹ txt íŒŒì¼ì—ì„œ ê°€ì¥ ì–¸ê¸‰ì´ ë§ì•˜ë˜ ë§ì•˜ë˜ ì£¼ì œ 5ê°€ì§€ë¡œ bulletìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤."""
        )

# Query text
query_text = st.text_input('ì§ˆë¬¸ ì…ë ¥í•˜ê¸°', placeholder = 'Ex. ìš”ì•½ì„ ì›í•  ê²½ìš°, ì•„ë˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë³µì‚¬, ë¶™ì—¬ë„£ê¸° í•´ì£¼ì„¸ìš”.', disabled=not uploaded_file)


# Form input and query
result = []
with st.form('ì¶”ê°€ì§ˆì˜', clear_on_submit=True):
    submitted = st.form_submit_button('ì…ë ¥', disabled=not(query_text))
    if submitted:
        with st.spinner('ë‹µë³€ ì¤€ë¹„ì¤‘..'):
            response = generate_response(new_file, query_text)
            result.append(response)

if len(result):
    st.info(response)



# ì—†ì–´ë„ ë˜ëŠ”ê±°
st.sidebar.title("About")

st.sidebar.info(
        "ì¹´ì¹´ì˜¤í†¡ ë‚´ìš© ìš”ì•½í•´ë“œë¦¼"
    )