import re
import datetime
import streamlit as st
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate


# txtíŒŒì¼ ìë¥´ê¸° í•¨ìˆ˜; input: ì¹´ì¹´ì˜¤í†¡ txtíŒŒì¼(type: UploadedFile), output: í•˜ë£¨ì¹˜ txt íŒŒì¼ (type: Str)
def make_new_file(uploaded_document):
    # 1. íŒŒì¼ì„ ì½ì–´ì¤€ë‹¤.
    file = uploaded_document.read().decode("utf-8", errors="ignore")
    # 2. ëª©í‘œí•˜ëŠ” ë‚ ì§œ(í˜„ì¬ë¡œë¶€í„° í•˜ë£¨ ì „)ë¥¼ ì •ì˜í•œë‹¤. ì¹´ì¹´ì˜¤í†¡ txtíŒŒì¼ì€ ~ë…„ ~ì›” ~ì¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ.
    current_date = datetime.datetime.now()
    target_date = current_date - datetime.timedelta(days=1)
    target_date_str = f"{target_date.year}ë…„ {target_date.month}ì›” {target_date.day}ì¼"
    # 3.1. í•˜ë£¨ ì „ ë‚´ìš©ì´ ìˆì„ ê²½ìš°, ì¸ë±ì‹±ì„ í†µí•´ new_contentì— ì €ì¥í•œë‹¤.
    if target_date_str in file:
        st.write(f"{target_date_str}ì˜ ê¸°ë¡ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        start_index = file.index(target_date_str)
        new_content = file[start_index:]
        #4. ì˜¤í”ˆì±„íŒ…ë°©ì— ìˆëŠ” ë¶ˆí•„ìš”í•˜ê²Œ ë°˜ë³µë˜ëŠ” [x|x|x] [ì˜¤í›„ x] ë¥¼ ì§€ì›Œì¤€ë‹¤ (ì—†ì„ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì¼€ì´ìŠ¤ êµ¬ë¶„)
        if  r'\[.*?\] \[\S+ \S+\] ':
            pattern = r'\[.*?\] \[\S+ \S+\] '
            cleaned_text = re.sub(pattern, '', new_content)
            return cleaned_text
        else:
            return new_content
    # 3.2. í•˜ë£¨ ì „ ë‚´ìš©ì´ ì—†ì„ ê²½ìš°, ë‚´ìš©ì´ ë‚˜ì˜¬ë•Œê¹Œì§€ í•˜ë£¨ì”© ë’¤ë¡œ ë¯¸ë£¬ë‹¤.  
    else:
        st.write(f"{target_date_str}ì˜ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        #ë‚´ìš©ì´ ìƒê¸¸ ë•Œê¹Œì§€ í•˜ë£¨ì”© ëº€ë‹¤.
        while target_date_str not in file:
            target_date -= datetime.timedelta(days=1)
            target_date_str = f"{target_date.year}ë…„ {target_date.month}ì›” {target_date.day}ì¼"
        #3.2.1. txtíŒŒì¼ ë‚´ì— ë‚ ì§œê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ” ê²½ìš°
        if target_date_str:
            st.write(f"{target_date_str}ì˜ ê¸°ë¡ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            start_index = file.index(target_date_str)
            new_content = file[start_index:]
            #4. ì˜¤í”ˆì±„íŒ…ë°©ì— ìˆëŠ” ë¶ˆí•„ìš”í•˜ê²Œ ë°˜ë³µë˜ëŠ” [x|x|x] [ì˜¤í›„ x] ë¥¼ ì§€ì›Œì¤€ë‹¤ (ì—†ì„ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì¼€ì´ìŠ¤ êµ¬ë¶„)
            if  r'\[.*?\] \[\S+ \S+\] ':
                pattern = r'\[.*?\] \[\S+ \S+\] '
                cleaned_text = re.sub(pattern, '', new_content)
                return cleaned_text
            else:
                return new_content
        #3.2.2. txtíŒŒì¼ ë‚´ì— ë‚ ì§œê°€ ì—†ëŠ” ê²½ìš°
        else:
             st.warning("ğŸš¨ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
# url ì¶”ì¶œ í•¨ìˆ˜; input: í•˜ë£¨ì¹˜ txt íŒŒì¼(Str), output: Str
def extract_urls(input_file):
    pattern = r'https://[^\s,]+'
    urls = re.findall(pattern, input_file)
    clickable_urls = '\n'.join([f"{index + 1}. [{url}]({url})" for index, url in enumerate(urls)])
    return clickable_urls


# Document ìƒì„± í•¨ìˆ˜; input: í•˜ë£¨ì¹˜ txt íŒŒì¼(type: Str), output: Document(type: Document(langchain))
def make_documents(new_file):
        # Split document into chunks -> strìœ¼ë¡œ êµ¬ì„±ëœ list ë§Œë“¦ -> text_splitter ë³€ìˆ˜ì— ì €ì¥
        text_splitter = CharacterTextSplitter(
             separator="\n",
             chunk_size=1000, #1000ìœ¼ë¡œ í–ˆëŠ”ë° ì ë‹¹í•œì§€ ëª¨ë¥´ê² ìŒ.
             chunk_overlap=20,
             length_function = len,
             is_separator_regex = False,
        )
        # Document[page_content, metadata]ì˜ ê°’ì„ textsì— ì €ì¥í•¨.
        texts = text_splitter.create_documents([new_file])
        # ë””ë²„ê¹… ì³Œ
        if not texts:
            raise ValueError("í…ìŠ¤íŠ¸ê°€ ë§Œë“¤ì–´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return texts


## ìš”ì•½ í•¨ìˆ˜ (ë§µë¦¬ë“€ìŠ¤ ì´ìš©): input: Document(type: Document(langchain)), output: í•µì‹¬ ì¸ì‚¬ì´íŠ¸(Str)
def summarize(Document, openai_api_key):
    template = """
    Don't say, just do:  
    '''
    í•´ë‹¹ ëŒ€í™” ë‚´ìš©ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
    {text}
    '''
    """
    combine_template ="""{text}
    ìš”ì•½ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì˜ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
    # ìš”ì•½
    ë‚´ìš©: ê° í…œí”Œë¦¿ì˜ ë¬¸ì¥
    ...
    """
    prompt = PromptTemplate(template=template, input_variables=["text"])
    template.format(text=Document)
    combine_prompt= PromptTemplate(template=combine_template, input_variables=["text"])
    chain = load_summarize_chain(llm=ChatOpenAI(openai_api_key=openai_api_key), 
                             map_prompt=prompt, 
                             combine_prompt=combine_prompt, 
                             chain_type='map_reduce', 
                             verbose=True)
    return chain.run(Document)



## QA Train í•¨ìˆ˜; input: Str, Str output: Str
def generate_response(texts, query_text, openai_api_key):
        embeddings=OpenAIEmbeddings(openai_api_key=openai_api_key)
        if not embeddings:
            raise ValueError("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
        # Create a vectorstore from documents using OpenAI Embeddings
        db = Chroma.from_documents(texts, embeddings)
        if not db:
            raise ValueError("db ìƒì„± ì‹¤íŒ¨")
        # Create retriever interface
        retriever = db.as_retriever()
        # Create QA chain chain type stuff ì¼ë‹¨ ìƒëµ
        qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(openai_api_key=openai_api_key, model_name='gpt-3.5-turbo'), retriever=retriever)
        if not qa:
            raise ValueError("QA Train ìƒì„± ì‹¤íŒ¨")
        return qa.run(query_text)